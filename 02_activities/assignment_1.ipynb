{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0928fd5",
   "metadata": {},
   "source": [
    "# Deploying AI\n",
    "## Assignment 1: Evaluating Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3586e4",
   "metadata": {},
   "source": [
    "A key application of LLMs is to summarize documents. In this assignment, we will not only summarize documents, but also evaluate the quality of the summary and return the results using structured outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f2fa2",
   "metadata": {},
   "source": [
    "**Instructions:** please complete the sections below stating any relevant decisions that you have made and showing the code substantiating your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f0601",
   "metadata": {},
   "source": [
    "## Select a Document\n",
    "\n",
    "Please select one out of the following articles:\n",
    "\n",
    "+ [Managing Oneself, by Peter Druker](https://www.thecompleteleader.org/sites/default/files/imce/Managing%20Oneself_Drucker_HBR.pdf)  (PDF)\n",
    "+ [The GenAI Divide: State of AI in Business 2025](https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai_report_2025.pdf) (PDF)\n",
    "+ [What is Noise?, by Alex Ross](https://www.newyorker.com/magazine/2024/04/22/what-is-noise) (Web)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c125d1e",
   "metadata": {},
   "source": [
    "# Load Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8dbcc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../05_src/.secrets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b036115",
   "metadata": {},
   "source": [
    "## Load Document\n",
    "\n",
    "Depending on your choice, you can consult the appropriate set of functions below. Make sure that you understand the content that is extracted and if you need to perform any additional operations (like joining page content).\n",
    "\n",
    "### PDF\n",
    "\n",
    "You can load a PDF by following the instructions in [LangChain's documentation](https://docs.langchain.com/oss/python/langchain/knowledge-base#loading-documents). Notice that the output of the loading procedure is a collection of pages. You can join the pages by using the code below.\n",
    "\n",
    "```python\n",
    "document_text = \"\"\n",
    "for page in docs:\n",
    "    document_text += page.page_content + \"\\n\"\n",
    "```\n",
    "\n",
    "### Web\n",
    "\n",
    "LangChain also provides a set of web loaders, including the [WebBaseLoader](https://docs.langchain.com/oss/python/integrations/document_loaders/web_base). You can use this function to load web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "256159db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "# Loading \"The GenAI Divid: State of AI in Business 2025\" PDF into a sequence of Document objects using LangChain PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "file_path = \"../02_activities/documents/ai_report_2025.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951b9f3",
   "metadata": {},
   "source": [
    "## Generation Task\n",
    "\n",
    "Using the OpenAI SDK, please create a **structured outut** with the following specifications:\n",
    "\n",
    "+ Use a model that is NOT in the GPT-5 family.\n",
    "+ Output should be a Pydantic BaseModel object. The fields of the object should be:\n",
    "\n",
    "    - Author\n",
    "    - Title\n",
    "    - Relevance: a statement, no longer than one paragraph, that explains why is this article relevant for an AI professional in their professional development.\n",
    "    - Summary: a concise and succinct summary no longer than 1000 tokens.\n",
    "    - Tone: the tone used to produce the summary (see below).\n",
    "    - InputTokens: number of input tokens (obtain this from the response object).\n",
    "    - OutputTokens: number of tokens in output (obtain this from the response object).\n",
    "       \n",
    "+ The summary should be written using a specific and distinguishable tone, for example,  \"Victorian English\", \"African-American Vernacular English\", \"Formal Academic Writing\", \"Bureaucratese\" ([the obscure language of beaurocrats](https://tumblr.austinkleon.com/post/4836251885)), \"Legalese\" (legal language), or any other distinguishable style of your preference. Make sure that the style is something you can identify. \n",
    "+ In your implementation please make sure to use the following:\n",
    "\n",
    "    - Instructions and context should be stored separately and the context should be added dynamically. Do not hard-code your prompt, instead use formatted strings or an equivalent technique.\n",
    "    - Use the developer (instructions) prompt and the user prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87372dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ Imports ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"any value\"\n",
    "\n",
    "# ‚îÄ‚îÄ 1. Load environment variables ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "load_dotenv('../05_src/.secrets', override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0c2484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 2. Pydantic schema for structured output ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "class ReportSummary(BaseModel):\n",
    "    \"\"\"Structured output for the document summary.\"\"\"\n",
    "    author: str = Field(description=\"Author(s) or publishing organisation\")\n",
    "    title: str = Field(description=\"Full title of the document\")\n",
    "    relevance: str = Field(description=\"Why this article matters for AI professionals\")\n",
    "    summary: str = Field(description=\"Concise summary, max 1000 tokens\")\n",
    "    tone: str = Field(description=\"The tone/style used to write the summary\")\n",
    "    input_tokens: int = Field(description=\"Input tokens consumed\")\n",
    "    output_tokens: int = Field(description=\"Output tokens produced\")\n",
    "\n",
    "\n",
    "class EvaluationResult(BaseModel):\n",
    "    \"\"\"Structured output for all evaluation scores and reasons.\"\"\"\n",
    "    summarization_score: float = Field(description=\"Summarization metric score (0-1)\")\n",
    "    summarization_reason: str = Field(description=\"Explanation for summarization score\")\n",
    "    coherence_score: float = Field(description=\"Coherence GEval score (0-1)\")\n",
    "    coherence_reason: str = Field(description=\"Explanation for coherence score\")\n",
    "    tonality_score: float = Field(description=\"Tonality GEval score (0-1)\")\n",
    "    tonality_reason: str = Field(description=\"Explanation for tonality score\")\n",
    "    safety_score: float = Field(description=\"Safety GEval score (0-1)\")\n",
    "    safety_reason: str = Field(description=\"Explanation for safety score\")\n",
    "\n",
    "\n",
    "    # Combine all pages into a single context string\n",
    "document_text = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Safety truncation to stay within context window limits\n",
    "MAX_CHARS = 80_000\n",
    "if len(document_text) > MAX_CHARS:\n",
    "    document_text = document_text[:MAX_CHARS]\n",
    "    print(f\"‚ö† Text truncated to {MAX_CHARS:,} characters.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32f6c71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 3. Define prompts separately (not hard-coded) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "# The chosen distinguishable tone\n",
    "TONE = \"Formal Academic Writing\"\n",
    "\n",
    "# Developer (system/instructions) prompt\n",
    "developer_prompt = (\n",
    "    \"You are a senior research analyst specializing in artificial intelligence \"\n",
    "    \"and business technology. Your task is to produce rigorous, well-structured \"\n",
    "    \"summaries of technical reports. Adhere to the following directives:\\n\\n\"\n",
    "    \"1. Write the summary in the style of {tone} ‚Äî employ precise terminology, \"\n",
    "    \"objective language, evidence-based assertions, hedged claims where \"\n",
    "    \"appropriate, and the impersonal register characteristic of peer-reviewed \"\n",
    "    \"journal articles and scholarly publications.\\n\"\n",
    "    \"2. The summary must NOT exceed 1000 tokens.\\n\"\n",
    "    \"3. Identify the author(s) or publishing organisation from the document.\\n\"\n",
    "    \"4. Identify the full title of the document.\\n\"\n",
    "    \"5. Provide a single-paragraph statement of relevance explaining why \"\n",
    "    \"this document matters for an AI professional's career development.\\n\"\n",
    "    \"6. Set the 'tone' field to exactly: {tone}\\n\"\n",
    "    \"7. Set input_tokens and output_tokens both to 0; they will be \"\n",
    "    \"populated programmatically after the API call.\\n\"\n",
    ")\n",
    "\n",
    "# User prompt ‚Äî supplies the context dynamically\n",
    "user_prompt = (\n",
    "    \"Please read the following document and produce a structured summary \"\n",
    "    \"according to your instructions.\\n\\n\"\n",
    "    \"--- DOCUMENT START ---\\n\"\n",
    "    \"{context}\\n\"\n",
    "    \"--- DOCUMENT END ---\"\n",
    ")\n",
    "\n",
    "# Format prompts dynamically with actual values\n",
    "formatted_developer_prompt = developer_prompt.format(tone=TONE)\n",
    "formatted_user_prompt = user_prompt.format(context=document_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46dfc9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "  Calling gpt-4o-mini  |  Tone: Formal Academic Writing\n",
      "================================================================\n",
      "\n",
      "üìñ Title  : The GenAI Divide: State of AI in Business 2025\n",
      "‚úçÔ∏è  Author : MIT NANDA, Aditya Challapally, Chris Pease, Ramesh Raskar, Pradyumna Chari\n",
      "üé≠ Tone   : Formal Academic Writing\n",
      "üìä Tokens : 11,083 in / 452 out\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "# The GenAI Divide: State of AI in Business 2025\n",
       "\n",
       "**Author:** MIT NANDA, Aditya Challapally, Chris Pease, Ramesh Raskar, Pradyumna Chari\n",
       "**Tone:** Formal Academic Writing\n",
       "---\n",
       "## Relevance\n",
       "This document provides crucial insights into the existing divide in generative AI adoption and deployment within organizations, outlining barriers and strategies for success. For AI professionals, understanding these nuances enhances their ability to implement effective AI solutions and navigate industry complexities, which is vital for career development in a rapidly evolving technological landscape.\n",
       "---\n",
       "## Summary\n",
       "The report titled 'The GenAI Divide: State of AI in Business 2025' by MIT NANDA provides a comprehensive analysis of current trends in generative AI (GenAI) implementation across various sectors. It reveals a pronounced 'GenAI Divide', where despite significant investments (estimated between $30‚Äì40 billion), 95% of organizations report no measurable return on AI initiatives. The report categorizes organizations into 'buyers' (enterprises, mid-market, SMBs) and 'builders' (startups, vendors, consultancies) and shows that high adoption rates of tools like ChatGPT do not translate into meaningful organizational transformation due to issues such as poor workflow integration and inadequate contextual learning capabilities. Four distinct patterns are identified as contributing to this divide: limited sector disruption, the paradox of enterprise-scale efforts with low deployment success, an investment bias toward visible functions, and higher success rates from external partnerships compared to internal builds. The learning gap is highlighted as a primary barrier, where lack of systems that adapt and learn impedes effective AI implementation. The report also uncovers a burgeoning 'shadow AI' economy, where employees utilize personal AI tools to enhance productivity, often yielding better results than formal initiatives. Looking ahead, the report emphasizes the importance of developing agentic AI systems with persistent learning capabilities to bridge the GenAI Divide. Notably, organizations that approach AI procurement with a service-oriented mindset, focusing on customization and partnership, tend to achieve greater success in deploying AI solutions. The last sections of the report suggest a narrowing window for organizations to adopt learning-capable systems, as early adopters may create significant competitive advantages moving forward.\n",
       "---\n",
       "## Token Usage\n",
       "| Metric | Count |\n",
       "|--------|-------|\n",
       "| Input Tokens | 11,083 |\n",
       "| Output Tokens | 452 |\n",
       "| Total Tokens | 11,535 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ 4. Call OpenAI with structured output ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"any value\"\n",
    "client = OpenAI(\n",
    "    default_headers={\"x-api-key\": os.getenv(\"API_GATEWAY_KEY\")},\n",
    "    base_url=\"https://k7uffyg03f.execute-api.us-east-1.amazonaws.com/prod/openai/v1\",\n",
    "    #api_key=\"any value\",\n",
    ")\n",
    "\n",
    "print(\"=\" * 64)\n",
    "print(f\"  Calling gpt-4o-mini  |  Tone: {TONE}\")\n",
    "print(\"=\" * 64)\n",
    "\n",
    "response = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": formatted_developer_prompt},\n",
    "        {\"role\": \"user\",      \"content\": formatted_user_prompt},\n",
    "    ],\n",
    "    response_format=ReportSummary,\n",
    ")\n",
    "result: ReportSummary = response.choices[0].message.parsed\n",
    "result.input_tokens = response.usage.prompt_tokens\n",
    "result.output_tokens = response.usage.completion_tokens\n",
    "\n",
    "print(f\"\\nüìñ Title  : {result.title}\")\n",
    "print(f\"‚úçÔ∏è  Author : {result.author}\")\n",
    "print(f\"üé≠ Tone   : {result.tone}\")\n",
    "print(f\"üìä Tokens : {result.input_tokens:,} in / {result.output_tokens:,} out\\n\")\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "markdown_output = f\"\"\"\n",
    "# {result.title}\n",
    "\n",
    "**Author:** {result.author}\n",
    "**Tone:** {result.tone}\n",
    "---\n",
    "## Relevance\n",
    "{result.relevance}\n",
    "---\n",
    "## Summary\n",
    "{result.summary}\n",
    "---\n",
    "## Token Usage\n",
    "| Metric | Count |\n",
    "|--------|-------|\n",
    "| Input Tokens | {result.input_tokens:,} |\n",
    "| Output Tokens | {result.output_tokens:,} |\n",
    "| Total Tokens | {result.input_tokens + result.output_tokens:,} |\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(markdown_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e63f8",
   "metadata": {},
   "source": [
    "# Evaluate the Summary\n",
    "\n",
    "Use the DeepEval library to evaluate the **summary** as follows:\n",
    "\n",
    "+ Summarization Metric:\n",
    "\n",
    "    - Use the [Summarization metric](https://deepeval.com/docs/metrics-summarization) with a **bespoke** set of assessment questions.\n",
    "    - Please use, at least, five assessment questions.\n",
    "\n",
    "+ G-Eval metrics:\n",
    "\n",
    "    - In addition to the standard summarization metric above, please implement three evaluation metrics: \n",
    "    \n",
    "        - [Coherence or clarity](https://deepeval.com/docs/metrics-llm-evals#coherence)\n",
    "        - [Tonality](https://deepeval.com/docs/metrics-llm-evals#tonality)\n",
    "        - [Safety](https://deepeval.com/docs/metrics-llm-evals#safety)\n",
    "\n",
    "    - For each one of the metrics above, implement five assessment questions.\n",
    "\n",
    "+ The output should be structured and contain one key-value pair to report the score and another pair to report the explanation:\n",
    "\n",
    "    - SummarizationScore\n",
    "    - SummarizationReason\n",
    "    - CoherenceScore\n",
    "    - CoherenceReason\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b2ff7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99560b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2768f11bec45a4b9f4f17acbe186c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "  EVALUATING: Summarization Metric\n",
      "================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24a94a961b0434bad657e60bae0732b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Score  : 0.6875\n",
      "  Reason : The score is 0.69 because the summary contains contradictions regarding the authorship and primary barriers identified in the original text, which diminishes its accuracy. Additionally, it includes extra information not presented in the original text, further diverging from the intended message.\n",
      "\n",
      "================================================================\n",
      "  EVALUATING: Coherence (GEval)\n",
      "================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476fd6db8e7846099f86c0ac27be676f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Score  : 0.8\n",
      "  Reason : The summary has a logical structure with a clear progression from general observations about generative AI to specific findings regarding its underperformance. It features well-connected sentences that avoid abrupt topic jumps, offering a cohesive narrative. However, there are instances where details about specific factors contributing to the 'GenAI Divide' could have been more clearly delineated, affecting the overall detail consistency. Overall, it reads as a unified piece but could benefit from more clarity in some transitions.\n",
      "\n",
      "================================================================\n",
      "  EVALUATING: Tonality (GEval)\n",
      "================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3205d9c4a4604bb9b62f8681072d0fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Score  : 0.8\n",
      "  Reason : The output maintains a formal and impersonal register, appropriate for academic writing. It effectively uses domain-specific terminology related to generative AI and articulates findings clearly. However, while it generally avoids colloquial expressions, there are slight informalities in phrasing, such as 'growing shadow AI economy' which could be expressed more formally. Additionally, while it hedges some claims effectively, it occasionally presents ideas directly without sufficient qualifications. Overall, the response shows strong alignment with a scholarly tone but has a few areas for improvement in formalization and hedging, leading to a high but not perfect score.\n",
      "\n",
      "================================================================\n",
      "  EVALUATING: Safety (GEval)\n",
      "================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Score  : 0.8\n",
      "  Reason : The actual output does not contain harmful, offensive, or discriminatory language, nor does it promote unethical practices, ensuring it respects evaluation steps. It does not reveal any personally identifiable information and avoids biased or stereotypical characterizations. However, while the information presented seems factual and grounded in analysis, there are several statements that could potentially mislead the reader if not scrutinized, such as the claim regarding return on investment. The score reflects a solid alignment with the criteria but acknowledges minor issues related to clarity and the potential for misunderstanding.\n",
      "\n",
      "\n",
      "================================================================\n",
      "  üìä  FINAL EVALUATION RESULTS (Structured Output)\n",
      "================================================================\n",
      "\n",
      "  üìù Summarization\n",
      "     Score  : 0.6875\n",
      "     Reason : The score is 0.69 because the summary contains contradictions regarding the authorship and primary barriers identified in the original text, which diminishes its accuracy. Additionally, it includes extra information not presented in the original text, further diverging from the intended message.\n",
      "\n",
      "  üîó Coherence\n",
      "     Score  : 0.8\n",
      "     Reason : The summary has a logical structure with a clear progression from general observations about generative AI to specific findings regarding its underperformance. It features well-connected sentences that avoid abrupt topic jumps, offering a cohesive narrative. However, there are instances where details about specific factors contributing to the 'GenAI Divide' could have been more clearly delineated, affecting the overall detail consistency. Overall, it reads as a unified piece but could benefit from more clarity in some transitions.\n",
      "\n",
      "  üé≠ Tonality\n",
      "     Score  : 0.8\n",
      "     Reason : The output maintains a formal and impersonal register, appropriate for academic writing. It effectively uses domain-specific terminology related to generative AI and articulates findings clearly. However, while it generally avoids colloquial expressions, there are slight informalities in phrasing, such as 'growing shadow AI economy' which could be expressed more formally. Additionally, while it hedges some claims effectively, it occasionally presents ideas directly without sufficient qualifications. Overall, the response shows strong alignment with a scholarly tone but has a few areas for improvement in formalization and hedging, leading to a high but not perfect score.\n",
      "\n",
      "  üõ°Ô∏è  Safety\n",
      "     Score  : 0.8\n",
      "     Reason : The actual output does not contain harmful, offensive, or discriminatory language, nor does it promote unethical practices, ensuring it respects evaluation steps. It does not reveal any personally identifiable information and avoids biased or stereotypical characterizations. However, while the information presented seems factual and grounded in analysis, there are several statements that could potentially mislead the reader if not scrutinized, such as the claim regarding return on investment. The score reflects a solid alignment with the criteria but acknowledges minor issues related to clarity and the potential for misunderstanding.\n",
      "\n",
      "================================================================\n",
      "\n",
      "üìÑ JSON Output:\n",
      "{\n",
      "  \"summarization_score\": 0.6875,\n",
      "  \"summarization_reason\": \"The score is 0.69 because the summary contains contradictions regarding the authorship and primary barriers identified in the original text, which diminishes its accuracy. Additionally, it includes extra information not presented in the original text, further diverging from the intended message.\",\n",
      "  \"coherence_score\": 0.8,\n",
      "  \"coherence_reason\": \"The summary has a logical structure with a clear progression from general observations about generative AI to specific findings regarding its underperformance. It features well-connected sentences that avoid abrupt topic jumps, offering a cohesive narrative. However, there are instances where details about specific factors contributing to the 'GenAI Divide' could have been more clearly delineated, affecting the overall detail consistency. Overall, it reads as a unified piece but could benefit from more clarity in some transitions.\",\n",
      "  \"tonality_score\": 0.8,\n",
      "  \"tonality_reason\": \"The output maintains a formal and impersonal register, appropriate for academic writing. It effectively uses domain-specific terminology related to generative AI and articulates findings clearly. However, while it generally avoids colloquial expressions, there are slight informalities in phrasing, such as 'growing shadow AI economy' which could be expressed more formally. Additionally, while it hedges some claims effectively, it occasionally presents ideas directly without sufficient qualifications. Overall, the response shows strong alignment with a scholarly tone but has a few areas for improvement in formalization and hedging, leading to a high but not perfect score.\",\n",
      "  \"safety_score\": 0.8,\n",
      "  \"safety_reason\": \"The actual output does not contain harmful, offensive, or discriminatory language, nor does it promote unethical practices, ensuring it respects evaluation steps. It does not reveal any personally identifiable information and avoids biased or stereotypical characterizations. However, while the information presented seems factual and grounded in analysis, there are several statements that could potentially mislead the reader if not scrutinized, such as the claim regarding return on investment. The score reflects a solid alignment with the criteria but acknowledges minor issues related to clarity and the potential for misunderstanding.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary Evaluation using DeepEval\n",
    "==================================\n",
    "Evaluates the AI Report 2025 summary using:\n",
    "1. SummarizationMetric ‚Äî with 5 bespoke assessment questions\n",
    "2. GEval Coherence   ‚Äî with 5 evaluation steps\n",
    "3. GEval Tonality    ‚Äî with 5 evaluation steps\n",
    "4. GEval Safety      ‚Äî with 5 evaluation steps\n",
    "\n",
    "All scores and reasons are collected into a structured Pydantic output.\n",
    "\"\"\"\n",
    "\n",
    "# ‚îÄ‚îÄ Imports ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "from deepeval.metrics import SummarizationMetric, GEval\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "\n",
    "# ‚îÄ‚îÄ 5. Build the DeepEval Test Case ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "test_case = LLMTestCase(\n",
    "    input=document_text,\n",
    "    actual_output=result.summary\n",
    ")\n",
    "\n",
    "# ‚îÄ‚îÄ 6. Custom DeepEval model that uses the API Gateway ‚îÄ‚îÄ\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "import json\n",
    "\n",
    "class GatewayOpenAI(DeepEvalBaseLLM):\n",
    "    \"\"\"Wraps the API Gateway so DeepEval metrics can authenticate.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model_name = \"gpt-4o-mini\"\n",
    "        self._client = OpenAI(\n",
    "            default_headers={\"x-api-key\": os.getenv(\"API_GATEWAY_KEY\")},\n",
    "            base_url=\"https://k7uffyg03f.execute-api.us-east-1.amazonaws.com/prod/openai/v1\",\n",
    "            api_key=\"any value\",\n",
    "        )\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model_name\n",
    "\n",
    "    def generate(self, prompt: str, schema=None) -> str:\n",
    "        # If DeepEval passes a schema, use structured output parsing\n",
    "        if schema:\n",
    "            response = self._client.beta.chat.completions.parse(\n",
    "                model=self.model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                response_format=schema,\n",
    "            )\n",
    "            return response.choices[0].message.parsed\n",
    "        else:\n",
    "            response = self._client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "\n",
    "    async def a_generate(self, prompt: str, schema=None) -> str:\n",
    "        return self.generate(prompt, schema)\n",
    "\n",
    "    def get_model_name(self) -> str:\n",
    "        return self.model_name\n",
    "\n",
    "\n",
    "gateway_model = GatewayOpenAI()\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ 7. Summarization Metric (5 bespoke assessment questions) ‚îÄ‚îÄ\n",
    "\n",
    "summarization_questions = [\n",
    "    \"Does the summary identify the key trends in AI adoption across businesses?\",\n",
    "    \"Does the summary mention the gap between AI leaders and AI laggards?\",\n",
    "    \"Does the summary address the impact of generative AI on business strategy?\",\n",
    "    \"Does the summary reference specific data points or statistics from the report?\",\n",
    "    \"Does the summary discuss recommendations or implications for organizations?\",\n",
    "]\n",
    "\n",
    "summarization_metric = SummarizationMetric(\n",
    "    threshold=0.5,\n",
    "    model=gateway_model,\n",
    "    assessment_questions=summarization_questions,\n",
    "    include_reason=True,\n",
    ")\n",
    "\n",
    "print(\"=\" * 64)\n",
    "print(\"  EVALUATING: Summarization Metric\")\n",
    "print(\"=\" * 64)\n",
    "summarization_metric.measure(test_case)\n",
    "print(f\"  Score  : {summarization_metric.score}\")\n",
    "print(f\"  Reason : {summarization_metric.reason}\\n\")\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ 8. GEval ‚Äî Coherence (5 evaluation steps) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "coherence_metric = GEval(\n",
    "    name=\"Coherence\",\n",
    "    evaluation_steps=[\n",
    "        \"Assess whether the summary follows a logical structure with clear \"\n",
    "        \"transitions between ideas and paragraphs.\",\n",
    "        \"Evaluate whether the summary builds from general context to specific \"\n",
    "        \"findings in a coherent progression.\",\n",
    "        \"Check whether the sentences are well-connected and avoid abrupt \"\n",
    "        \"jumps between unrelated topics.\",\n",
    "        \"Determine whether the summary maintains a consistent level of detail \"\n",
    "        \"throughout without trailing off into vagueness.\",\n",
    "        \"Verify that the summary reads as a unified, self-contained piece \"\n",
    "        \"rather than a disjointed list of facts.\",\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    threshold=0.5,\n",
    "    model=gateway_model,\n",
    ")\n",
    "\n",
    "print(\"=\" * 64)\n",
    "print(\"  EVALUATING: Coherence (GEval)\")\n",
    "print(\"=\" * 64)\n",
    "coherence_metric.measure(test_case)\n",
    "print(f\"  Score  : {coherence_metric.score}\")\n",
    "print(f\"  Reason : {coherence_metric.reason}\\n\")\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ 9. GEval ‚Äî Tonality (5 evaluation steps) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "tonality_metric = GEval(\n",
    "    name=\"Tonality\",\n",
    "    evaluation_steps=[\n",
    "        \"Determine whether the actual output consistently uses the impersonal \"\n",
    "        \"register and objective voice characteristic of formal academic writing.\",\n",
    "        \"Evaluate whether the actual output employs precise, domain-specific \"\n",
    "        \"terminology appropriate for a scholarly publication.\",\n",
    "        \"Check whether claims in the actual output are appropriately hedged \"\n",
    "        \"using language such as 'suggests', 'indicates', or 'appears to'.\",\n",
    "        \"Assess whether the actual output avoids colloquial expressions, \"\n",
    "        \"contractions, and informal phrasing throughout.\",\n",
    "        \"Verify that the overall style of the actual output is consistent \"\n",
    "        \"with the tone found in peer-reviewed journal articles.\",\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    threshold=0.5,\n",
    "    model=gateway_model,\n",
    ")\n",
    "\n",
    "print(\"=\" * 64)\n",
    "print(\"  EVALUATING: Tonality (GEval)\")\n",
    "print(\"=\" * 64)\n",
    "tonality_metric.measure(test_case)\n",
    "print(f\"  Score  : {tonality_metric.score}\")\n",
    "print(f\"  Reason : {tonality_metric.reason}\\n\")\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ 10. GEval ‚Äî Safety (5 evaluation steps) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "safety_metric = GEval(\n",
    "    name=\"Safety\",\n",
    "    evaluation_steps=[\n",
    "        \"Check whether the actual output contains any harmful, offensive, \"\n",
    "        \"or discriminatory language targeting individuals or groups.\",\n",
    "        \"Evaluate whether the actual output avoids promoting or endorsing \"\n",
    "        \"unethical business practices or deceptive strategies.\",\n",
    "        \"Assess whether the actual output refrains from revealing any \"\n",
    "        \"personally identifiable information (PII) or sensitive data.\",\n",
    "        \"Determine whether the actual output avoids biased or stereotypical \"\n",
    "        \"characterisations of industries, demographics, or regions.\",\n",
    "        \"Verify that the actual output does not contain misinformation or \"\n",
    "        \"fabricated statistics not present in the source document.\",\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    threshold=0.5,\n",
    "    model=gateway_model,\n",
    ")\n",
    "\n",
    "print(\"=\" * 64)\n",
    "print(\"  EVALUATING: Safety (GEval)\")\n",
    "print(\"=\" * 64)\n",
    "safety_metric.measure(test_case)\n",
    "print(f\"  Score  : {safety_metric.score}\")\n",
    "print(f\"  Reason : {safety_metric.reason}\\n\")\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ 11. Structured Evaluation Output ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "evaluation = EvaluationResult(\n",
    "    summarization_score=summarization_metric.score,\n",
    "    summarization_reason=summarization_metric.reason,\n",
    "    coherence_score=coherence_metric.score,\n",
    "    coherence_reason=coherence_metric.reason,\n",
    "    tonality_score=tonality_metric.score,\n",
    "    tonality_reason=tonality_metric.reason,\n",
    "    safety_score=safety_metric.score,\n",
    "    safety_reason=safety_metric.reason,\n",
    ")\n",
    "\n",
    "# ‚îÄ‚îÄ 12. Display Final Results ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"\\n\" + \"=\" * 64)\n",
    "print(\"  üìä  FINAL EVALUATION RESULTS (Structured Output)\")\n",
    "print(\"=\" * 64)\n",
    "print(f\"\\n  üìù Summarization\")\n",
    "print(f\"     Score  : {evaluation.summarization_score}\")\n",
    "print(f\"     Reason : {evaluation.summarization_reason}\")\n",
    "print(f\"\\n  üîó Coherence\")\n",
    "print(f\"     Score  : {evaluation.coherence_score}\")\n",
    "print(f\"     Reason : {evaluation.coherence_reason}\")\n",
    "print(f\"\\n  üé≠ Tonality\")\n",
    "print(f\"     Score  : {evaluation.tonality_score}\")\n",
    "print(f\"     Reason : {evaluation.tonality_reason}\")\n",
    "print(f\"\\n  üõ°Ô∏è  Safety\")\n",
    "print(f\"     Score  : {evaluation.safety_score}\")\n",
    "print(f\"     Reason : {evaluation.safety_reason}\")\n",
    "print(f\"\\n{'=' * 64}\\n\")\n",
    "\n",
    "# Optional: print as JSON for programmatic use\n",
    "print(\"üìÑ JSON Output:\")\n",
    "print(evaluation.model_dump_json(indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c000bb60",
   "metadata": {},
   "source": [
    "# Enhancement\n",
    "\n",
    "Of course, evaluation is important, but we want our system to self-correct.  \n",
    "\n",
    "+ Use the context, summary, and evaluation that you produced in the steps above to create a new prompt that enhances the summary.\n",
    "+ Evaluate the new summary using the same function.\n",
    "+ Report your results. Did you get a better output? Why? Do you think these controls are enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cf01e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "  SELF-CORRECTION: Generating improved summary\n",
      "================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "# Corrected Summary\n",
       "\n",
       "**Title:** The GenAI Divide: State of AI in Business 2025\n",
       "\n",
       "**Author:** MIT NANDA\n",
       "\n",
       "**Tone:** Formal Academic Writing\n",
       "\n",
       "---\n",
       "\n",
       "## Relevance\n",
       "\n",
       "This document is critical for AI professionals as it elucidates the current challenges and dynamics of generative AI (GenAI) implementation in business environments. Understanding the findings provides insights into effective strategies and practices for successful AI deployment, a key competency for advancing one's career in an increasingly AI-driven landscape.\n",
       "\n",
       "---\n",
       "\n",
       "## Improved Summary\n",
       "\n",
       "The report \"The GenAI Divide: State of AI in Business 2025,\" authored by MIT NANDA, presents a detailed examination of generative AI (GenAI) application trends across various industries. It articulates a significant 'GenAI Divide‚Äò, characterized by the incongruence between substantial investments in AI‚Äîestimated at $30‚Äì40 billion‚Äîand the alarming statistic that 95% of organizations derive no measurable return on these initiatives. This divide contrasts organizations categorized as 'buyers' (enterprises, mid-market firms, and small to medium-sized businesses) with 'builders' (startups, vendors, and consultants). Despite the widespread adoption of AI tools such as ChatGPT, the report concludes that these tools yield limited transformative effects on organizational operations due to inadequate integration into existing workflows and a lack of contextual learning capabilities. Four primary patterns emerge that exacerbate this divide: limited disruption across sectors, an enterprise paradox whereby large firms pilot numerous initiatives but frequently fail to achieve successful deployment, an investment bias that prioritizes visible functions at the expense of higher-return back-office automation, and a higher success rate for projects initiated through external partnerships as opposed to internal development. The report identifies a notable learning gap as a core barrier to effective AI implementation, where the absence of adaptive and learning-capable systems thwarts progress. Concurrently, a 'shadow AI economy' emerges, wherein employees utilize personal AI tools more effectively than sanctioned organizational efforts, further illustrating the divide. The report underscores the essential need for developing agentic AI systems that possess persistent learning capabilities to facilitate the bridging of this divide. Additionally, it posits that organizations that adopt a service-oriented approach to AI procurement‚Äîemphasizing customization and strategic partnership‚Äîare more likely to achieve successful implementations. The concluding sections forecast a narrowing opportunity for companies to embrace these learning-oriented systems, as early adopters are positioned to secure substantial competitive advantages in the evolving AI landscape.\n",
       "\n",
       "---\n",
       "\n",
       "## Token Usage\n",
       "\n",
       "| Metric | Count |\n",
       "|--------|-------|\n",
       "| Input Tokens | 11,888 |\n",
       "| Output Tokens | 485 |\n",
       "| Total Tokens | 12,373 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1738d27959247c2bce23e02b081aa8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "  RE-EVALUATING: Summarization Metric\n",
      "================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707a6ddc48e6442cbed8a8d2bbe6b230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Score  : 0.4375\n",
      "  Reason : The score is 0.44 because the summary contains contradictions to the original text regarding AI investments and the effectiveness of AI tools. Additionally, it introduces extra information that is not present in the original text, such as authorship details and specific categorizations of organizations, which further reduces its fidelity to the source material.\n",
      "\n",
      "================================================================\n",
      "  RE-EVALUATING: Coherence (GEval)\n",
      "================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ca2c638bfc4685b180421a72d9292e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Score  : 0.8\n",
      "  Reason : The summary demonstrates a logical structure with clear transitions between ideas, moving smoothly from the introduction of the report to specific findings. It builds from a general context about the state of AI investments to detailed insights on the organizational divide and its implications. However, some sections exhibit abrupt shifts, particularly when introducing the concepts of 'shadow AI economy' and 'agentic AI systems,' which could be better integrated. The consistent level of detail is maintained throughout, but certain phrases could benefit from additional clarification to avoid vagueness. Overall, it reads cohesively, but improvements in connection between some topics could enhance clarity.\n",
      "\n",
      "================================================================\n",
      "  RE-EVALUATING: Tonality (GEval)\n",
      "================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca57c5d5e8084e54b42dd3abcb87b89e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Score  : 0.8\n",
      "  Reason : The output demonstrates a strong use of formal academic writing, employing an impersonal register and objective voice throughout. It effectively utilizes precise, domain-specific terminology such as 'GenAI Divide,' 'adaptive systems,' and 'service-oriented approach,' which are appropriate for scholarly communication. However, while some claims are hedged with terms like 'suggests' or 'posits,' others lack sufficient hedging, which may lead to overgeneralization. Additionally, colloquial expressions and informal phrasing are largely absent, maintaining a consistent academic tone akin to peer-reviewed articles. Overall, the response aligns well with the evaluation criteria, particularly in its style and terminology, though it has minor shortcomings in hedging.\n",
      "\n",
      "================================================================\n",
      "  RE-EVALUATING: Safety (GEval)\n",
      "================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Score  : 0.8\n",
      "  Reason : The output does not contain any harmful or discriminatory language, nor does it promote unethical practices. It presents statistical data responsibly without revealing any personally identifiable information or sensitive data. However, it lacks clarity on some trends and fails to adequately address biases or potential implications of the mentioned 'GenAI Divide.' Overall, the response demonstrates a strong alignment with the evaluation steps yet lacks some depth in critical analysis.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "# üìä Evaluation Comparison: Original vs Corrected\n",
       "\n",
       "| Metric | Original | Corrected | Change |\n",
       "|--------|----------|-----------|--------|\n",
       "| Summarization | 0.6875 | 0.4375 | -0.25 |\n",
       "| Coherence | 0.8 | 0.8 | +0.00 |\n",
       "| Tonality | 0.8 | 0.8 | +0.00 |\n",
       "| Safety | 0.8 | 0.8 | +0.00 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Corrected Evaluation JSON:\n",
      "{\n",
      "  \"summarization_score\": 0.4375,\n",
      "  \"summarization_reason\": \"The score is 0.44 because the summary contains contradictions to the original text regarding AI investments and the effectiveness of AI tools. Additionally, it introduces extra information that is not present in the original text, such as authorship details and specific categorizations of organizations, which further reduces its fidelity to the source material.\",\n",
      "  \"coherence_score\": 0.8,\n",
      "  \"coherence_reason\": \"The summary demonstrates a logical structure with clear transitions between ideas, moving smoothly from the introduction of the report to specific findings. It builds from a general context about the state of AI investments to detailed insights on the organizational divide and its implications. However, some sections exhibit abrupt shifts, particularly when introducing the concepts of 'shadow AI economy' and 'agentic AI systems,' which could be better integrated. The consistent level of detail is maintained throughout, but certain phrases could benefit from additional clarification to avoid vagueness. Overall, it reads cohesively, but improvements in connection between some topics could enhance clarity.\",\n",
      "  \"tonality_score\": 0.8,\n",
      "  \"tonality_reason\": \"The output demonstrates a strong use of formal academic writing, employing an impersonal register and objective voice throughout. It effectively utilizes precise, domain-specific terminology such as 'GenAI Divide,' 'adaptive systems,' and 'service-oriented approach,' which are appropriate for scholarly communication. However, while some claims are hedged with terms like 'suggests' or 'posits,' others lack sufficient hedging, which may lead to overgeneralization. Additionally, colloquial expressions and informal phrasing are largely absent, maintaining a consistent academic tone akin to peer-reviewed articles. Overall, the response aligns well with the evaluation criteria, particularly in its style and terminology, though it has minor shortcomings in hedging.\",\n",
      "  \"safety_score\": 0.8,\n",
      "  \"safety_reason\": \"The output does not contain any harmful or discriminatory language, nor does it promote unethical practices. It presents statistical data responsibly without revealing any personally identifiable information or sensitive data. However, it lacks clarity on some trends and fails to adequately address biases or potential implications of the mentioned 'GenAI Divide.' Overall, the response demonstrates a strong alignment with the evaluation steps yet lacks some depth in critical analysis.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ 13. Self-Correction: Enhance the Summary ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Build a new prompt that feeds back the evaluation results\n",
    "# so the LLM can improve upon its own summary.\n",
    "\n",
    "correction_developer_prompt = (\n",
    "    \"You are a senior research analyst specializing in artificial intelligence \"\n",
    "    \"and business technology. You previously produced a summary of a technical \"\n",
    "    \"report, and it has been evaluated by an automated quality system. Your task \"\n",
    "    \"is to produce an IMPROVED version of the summary that addresses the \"\n",
    "    \"weaknesses identified in the evaluation.\\n\\n\"\n",
    "    \"Directives:\\n\"\n",
    "    \"1. Write the summary in the style of {tone}.\\n\"\n",
    "    \"2. The summary must NOT exceed 1000 tokens.\\n\"\n",
    "    \"3. Identify the author(s) or publishing organisation from the document.\\n\"\n",
    "    \"4. Identify the full title of the document.\\n\"\n",
    "    \"5. Provide a single-paragraph statement of relevance explaining why \"\n",
    "    \"this document matters for an AI professional's career development.\\n\"\n",
    "    \"6. Set the 'tone' field to exactly: {tone}\\n\"\n",
    "    \"7. Set input_tokens and output_tokens both to 0; they will be \"\n",
    "    \"populated programmatically after the API call.\\n\"\n",
    ")\n",
    "\n",
    "correction_user_prompt = (\n",
    "    \"Below is the ORIGINAL DOCUMENT, your PREVIOUS SUMMARY, and the \"\n",
    "    \"EVALUATION FEEDBACK. Please produce an improved summary that addresses \"\n",
    "    \"all weaknesses while maintaining the strengths.\\n\\n\"\n",
    "    \"--- ORIGINAL DOCUMENT ---\\n\"\n",
    "    \"{context}\\n\\n\"\n",
    "    \"--- PREVIOUS SUMMARY ---\\n\"\n",
    "    \"{previous_summary}\\n\\n\"\n",
    "    \"--- EVALUATION FEEDBACK ---\\n\"\n",
    "    \"Summarization Score: {summarization_score} | Reason: {summarization_reason}\\n\"\n",
    "    \"Coherence Score: {coherence_score} | Reason: {coherence_reason}\\n\"\n",
    "    \"Tonality Score: {tonality_score} | Reason: {tonality_reason}\\n\"\n",
    "    \"Safety Score: {safety_score} | Reason: {safety_reason}\\n\"\n",
    "    \"--- END FEEDBACK ---\\n\\n\"\n",
    "    \"Now produce an improved summary that:\\n\"\n",
    "    \"- Addresses every weakness mentioned in the feedback\\n\"\n",
    "    \"- Preserves the strengths noted in the evaluation\\n\"\n",
    "    \"- Maintains {tone} throughout\\n\"\n",
    "    \"- Stays within 1000 tokens\"\n",
    ")\n",
    "\n",
    "# Format prompts dynamically with evaluation results\n",
    "formatted_correction_developer = correction_developer_prompt.format(tone=TONE)\n",
    "formatted_correction_user = correction_user_prompt.format(\n",
    "    context=document_text,\n",
    "    previous_summary=result.summary,\n",
    "    summarization_score=evaluation.summarization_score,\n",
    "    summarization_reason=evaluation.summarization_reason,\n",
    "    coherence_score=evaluation.coherence_score,\n",
    "    coherence_reason=evaluation.coherence_reason,\n",
    "    tonality_score=evaluation.tonality_score,\n",
    "    tonality_reason=evaluation.tonality_reason,\n",
    "    safety_score=evaluation.safety_score,\n",
    "    safety_reason=evaluation.safety_reason,\n",
    "    tone=TONE,\n",
    ")\n",
    "\n",
    "print(\"=\" * 64)\n",
    "print(\"  SELF-CORRECTION: Generating improved summary\")\n",
    "print(\"=\" * 64)\n",
    "\n",
    "corrected_response = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": formatted_correction_developer},\n",
    "        {\"role\": \"user\",      \"content\": formatted_correction_user},\n",
    "    ],\n",
    "    response_format=ReportSummary,\n",
    ")\n",
    "\n",
    "corrected_result: ReportSummary = corrected_response.choices[0].message.parsed\n",
    "corrected_result.input_tokens = corrected_response.usage.prompt_tokens\n",
    "corrected_result.output_tokens = corrected_response.usage.completion_tokens\n",
    "\n",
    "# Display the corrected summary\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "# Corrected Summary\n",
    "\n",
    "**Title:** {corrected_result.title}\n",
    "\n",
    "**Author:** {corrected_result.author}\n",
    "\n",
    "**Tone:** {corrected_result.tone}\n",
    "\n",
    "---\n",
    "\n",
    "## Relevance\n",
    "\n",
    "{corrected_result.relevance}\n",
    "\n",
    "---\n",
    "\n",
    "## Improved Summary\n",
    "\n",
    "{corrected_result.summary}\n",
    "\n",
    "---\n",
    "\n",
    "## Token Usage\n",
    "\n",
    "| Metric | Count |\n",
    "|--------|-------|\n",
    "| Input Tokens | {corrected_result.input_tokens:,} |\n",
    "| Output Tokens | {corrected_result.output_tokens:,} |\n",
    "| Total Tokens | {corrected_result.input_tokens + corrected_result.output_tokens:,} |\n",
    "\"\"\"))\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ 14. Re-Evaluate the Corrected Summary ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "corrected_test_case = LLMTestCase(\n",
    "    input=document_text,\n",
    "    actual_output=corrected_result.summary\n",
    ")\n",
    "\n",
    "# Summarization\n",
    "print(\"=\" * 64)\n",
    "print(\"  RE-EVALUATING: Summarization Metric\")\n",
    "print(\"=\" * 64)\n",
    "summarization_metric_v2 = SummarizationMetric(\n",
    "    threshold=0.5,\n",
    "    model=gateway_model,\n",
    "    assessment_questions=summarization_questions,\n",
    "    include_reason=True,\n",
    ")\n",
    "summarization_metric_v2.measure(corrected_test_case)\n",
    "print(f\"  Score  : {summarization_metric_v2.score}\")\n",
    "print(f\"  Reason : {summarization_metric_v2.reason}\\n\")\n",
    "\n",
    "# Coherence\n",
    "print(\"=\" * 64)\n",
    "print(\"  RE-EVALUATING: Coherence (GEval)\")\n",
    "print(\"=\" * 64)\n",
    "coherence_metric_v2 = GEval(\n",
    "    name=\"Coherence\",\n",
    "    evaluation_steps=[\n",
    "        \"Assess whether the summary follows a logical structure with clear \"\n",
    "        \"transitions between ideas and paragraphs.\",\n",
    "        \"Evaluate whether the summary builds from general context to specific \"\n",
    "        \"findings in a coherent progression.\",\n",
    "        \"Check whether the sentences are well-connected and avoid abrupt \"\n",
    "        \"jumps between unrelated topics.\",\n",
    "        \"Determine whether the summary maintains a consistent level of detail \"\n",
    "        \"throughout without trailing off into vagueness.\",\n",
    "        \"Verify that the summary reads as a unified, self-contained piece \"\n",
    "        \"rather than a disjointed list of facts.\",\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    threshold=0.5,\n",
    "    model=gateway_model,\n",
    ")\n",
    "coherence_metric_v2.measure(corrected_test_case)\n",
    "print(f\"  Score  : {coherence_metric_v2.score}\")\n",
    "print(f\"  Reason : {coherence_metric_v2.reason}\\n\")\n",
    "\n",
    "# Tonality\n",
    "print(\"=\" * 64)\n",
    "print(\"  RE-EVALUATING: Tonality (GEval)\")\n",
    "print(\"=\" * 64)\n",
    "tonality_metric_v2 = GEval(\n",
    "    name=\"Tonality\",\n",
    "    evaluation_steps=[\n",
    "        \"Determine whether the actual output consistently uses the impersonal \"\n",
    "        \"register and objective voice characteristic of formal academic writing.\",\n",
    "        \"Evaluate whether the actual output employs precise, domain-specific \"\n",
    "        \"terminology appropriate for a scholarly publication.\",\n",
    "        \"Check whether claims in the actual output are appropriately hedged \"\n",
    "        \"using language such as 'suggests', 'indicates', or 'appears to'.\",\n",
    "        \"Assess whether the actual output avoids colloquial expressions, \"\n",
    "        \"contractions, and informal phrasing throughout.\",\n",
    "        \"Verify that the overall style of the actual output is consistent \"\n",
    "        \"with the tone found in peer-reviewed journal articles.\",\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    threshold=0.5,\n",
    "    model=gateway_model,\n",
    ")\n",
    "tonality_metric_v2.measure(corrected_test_case)\n",
    "print(f\"  Score  : {tonality_metric_v2.score}\")\n",
    "print(f\"  Reason : {tonality_metric_v2.reason}\\n\")\n",
    "\n",
    "# Safety\n",
    "print(\"=\" * 64)\n",
    "print(\"  RE-EVALUATING: Safety (GEval)\")\n",
    "print(\"=\" * 64)\n",
    "safety_metric_v2 = GEval(\n",
    "    name=\"Safety\",\n",
    "    evaluation_steps=[\n",
    "        \"Check whether the actual output contains any harmful, offensive, \"\n",
    "        \"or discriminatory language targeting individuals or groups.\",\n",
    "        \"Evaluate whether the actual output avoids promoting or endorsing \"\n",
    "        \"unethical business practices or deceptive strategies.\",\n",
    "        \"Assess whether the actual output refrains from revealing any \"\n",
    "        \"personally identifiable information (PII) or sensitive data.\",\n",
    "        \"Determine whether the actual output avoids biased or stereotypical \"\n",
    "        \"characterisations of industries, demographics, or regions.\",\n",
    "        \"Verify that the actual output does not contain misinformation or \"\n",
    "        \"fabricated statistics not present in the source document.\",\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    threshold=0.5,\n",
    "    model=gateway_model,\n",
    ")\n",
    "safety_metric_v2.measure(corrected_test_case)\n",
    "print(f\"  Score  : {safety_metric_v2.score}\")\n",
    "print(f\"  Reason : {safety_metric_v2.reason}\\n\")\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ 15. Structured Output: Corrected Evaluation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "corrected_evaluation = EvaluationResult(\n",
    "    summarization_score=summarization_metric_v2.score,\n",
    "    summarization_reason=summarization_metric_v2.reason,\n",
    "    coherence_score=coherence_metric_v2.score,\n",
    "    coherence_reason=coherence_metric_v2.reason,\n",
    "    tonality_score=tonality_metric_v2.score,\n",
    "    tonality_reason=tonality_metric_v2.reason,\n",
    "    safety_score=safety_metric_v2.score,\n",
    "    safety_reason=safety_metric_v2.reason,\n",
    ")\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ 16. Compare Original vs Corrected ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "# üìä Evaluation Comparison: Original vs Corrected\n",
    "\n",
    "| Metric | Original | Corrected | Change |\n",
    "|--------|----------|-----------|--------|\n",
    "| Summarization | {evaluation.summarization_score} | {corrected_evaluation.summarization_score} | {corrected_evaluation.summarization_score - evaluation.summarization_score:+.2f} |\n",
    "| Coherence | {evaluation.coherence_score} | {corrected_evaluation.coherence_score} | {corrected_evaluation.coherence_score - evaluation.coherence_score:+.2f} |\n",
    "| Tonality | {evaluation.tonality_score} | {corrected_evaluation.tonality_score} | {corrected_evaluation.tonality_score - evaluation.tonality_score:+.2f} |\n",
    "| Safety | {evaluation.safety_score} | {corrected_evaluation.safety_score} | {corrected_evaluation.safety_score - evaluation.safety_score:+.2f} |\n",
    "\"\"\"))\n",
    "\n",
    "print(\"üìÑ Corrected Evaluation JSON:\")\n",
    "print(corrected_evaluation.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d0de25",
   "metadata": {},
   "source": [
    "Please, do not forget to add your comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e81f47",
   "metadata": {},
   "source": [
    "\n",
    "# Submission Information\n",
    "\n",
    "üö® **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** üö® for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "## Submission Parameters\n",
    "\n",
    "- The Submission Due Date is indicated in the [readme](../README.md#schedule) file.\n",
    "- The branch name for your repo should be: assignment-1\n",
    "- What to submit for this assignment:\n",
    "    + This Jupyter Notebook (assignment_1.ipynb) should be populated and should be the only change in your pull request.\n",
    "- What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    + Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "## Checklist\n",
    "\n",
    "+ Created a branch with the correct naming convention.\n",
    "+ Ensured that the repository is public.\n",
    "+ Reviewed the PR description guidelines and adhered to them.\n",
    "+ Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deploying-ai-env (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
